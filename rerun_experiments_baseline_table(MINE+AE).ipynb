{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07bb76bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the discrete case\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d09fc2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def make_gmm(n_components=10, dim=2, seed=42,\n",
    "             mean_range=(-5.0, 5.0), var_range=(0.05, 1.0),\n",
    "             isotropic=True):\n",
    "    \"\"\"\n",
    "    Build parameters for a Gaussian Mixture Model (GMM).\n",
    "\n",
    "    - n_components: number of Gaussian centers (clusters)\n",
    "    - dim: dimensionality (2 by default)\n",
    "    - seed: RNG seed for reproducibility\n",
    "    - mean_range: range for sampling means uniformly per dimension\n",
    "    - var_range: range for sampling variances (sigma^2); used if isotropic=True\n",
    "    - isotropic: if True, use scalar variance per component (sigma^2 * I).\n",
    "                 if False, generate random full SPD covariances.\n",
    "\n",
    "    Returns dict with keys: weights, means, covs, rng\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    # Mixture weights (sum to 1)\n",
    "    weights = rng.dirichlet(np.ones(n_components))\n",
    "\n",
    "    # Means\n",
    "    means = rng.uniform(mean_range[0], mean_range[1], size=(n_components, dim))\n",
    "\n",
    "    # Covariances\n",
    "    if isotropic:\n",
    "        # One random variance per component (same across dimensions)\n",
    "        variances = rng.uniform(var_range[0], var_range[1], size=n_components)\n",
    "        covs = np.array([v * np.eye(dim) for v in variances])\n",
    "    else:\n",
    "        # Random full SPD covariances with average variance within var_range\n",
    "        covs = []\n",
    "        target_var = rng.uniform(var_range[0], var_range[1], size=n_components)\n",
    "        for tvar in target_var:\n",
    "            A = rng.normal(size=(dim, dim))\n",
    "            C = A @ A.T + 1e-3 * np.eye(dim)  # SPD\n",
    "            # Scale so average variance (trace/dim) matches tvar\n",
    "            scale = tvar / (np.trace(C) / dim)\n",
    "            covs.append(C * scale)\n",
    "        covs = np.array(covs)\n",
    "\n",
    "    return dict(weights=weights, means=means, covs=covs, rng=rng)\n",
    "\n",
    "def sample_gmm(n_samples, weights, means, covs, rng=None):\n",
    "    \"\"\"\n",
    "    Sample points from a GMM described by weights, means, covs.\n",
    "    Returns (X, component_indices)\n",
    "    \"\"\"\n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng()\n",
    "\n",
    "    K = len(weights)\n",
    "    dim = means.shape[1]\n",
    "\n",
    "    # Choose component for each sample\n",
    "    comps = rng.choice(K, size=n_samples, p=weights)\n",
    "\n",
    "    # Draw samples\n",
    "    X = np.empty((n_samples, dim))\n",
    "    for k in range(K):\n",
    "        idx = np.where(comps == k)[0]\n",
    "        if idx.size > 0:\n",
    "            X[idx] = rng.multivariate_normal(mean=means[k], cov=covs[k], size=idx.size)\n",
    "    return X, comps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd28b693",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dataset = []\n",
    "\n",
    "params = make_gmm(n_components=20, dim=2, seed=123, mean_range=(-6, 6), var_range=(0.2, 0.8), isotropic=True)\n",
    "X, z = sample_gmm(n_samples=20000, **params)\n",
    "train_data = X-X.min()\n",
    "train_data = train_data/train_data.max()\n",
    "\n",
    "save_dataset.append(train_data)\n",
    "\n",
    "params = make_gmm(n_components=20, dim=2, seed=127, mean_range=(-6, 6), var_range=(0.2, 0.8), isotropic=True)\n",
    "X, z = sample_gmm(n_samples=20000, **params)\n",
    "train_data = X-X.min()\n",
    "train_data = train_data/train_data.max()\n",
    "\n",
    "save_dataset.append(train_data)\n",
    "\n",
    "params = make_gmm(n_components=20, dim=2, seed=128, mean_range=(-6, 6), var_range=(0.2, 0.8), isotropic=True)\n",
    "X, z = sample_gmm(n_samples=20000, **params)\n",
    "train_data = X-X.min()\n",
    "train_data = train_data/train_data.max()\n",
    "\n",
    "save_dataset.append(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4071cdd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BELOW IS KICA\n",
    "\n",
    "from scipy.linalg import eigh\n",
    "\n",
    "def negative_half_power(RX):\n",
    "    E1,V1 = torch.linalg.eigh(RX)\n",
    "    RF_NORM = V1@torch.diag(E1**(-1/2))@V1.T\n",
    "    return RF_NORM\n",
    "\n",
    "def compute_gram_matrix(X, Y, sigma):\n",
    "    \"\"\"Compute the Gram matrix between datasets X and Y using the Gaussian kernel.\"\"\"\n",
    "    # Compute pairwise squared distances between X and Y in a vectorized manner\n",
    "    pairwise_sq_dists = np.sum(X**2, axis=1).reshape(-1, 1) + np.sum(Y**2, axis=1) - 2 * np.dot(X, Y.T)\n",
    "    \n",
    "    # Compute the Gram matrix\n",
    "    gram_matrix = np.exp(-pairwise_sq_dists / (2 * sigma**2))\n",
    "    return gram_matrix\n",
    "\n",
    "def gaussian_kernel(x, y, sigma):\n",
    "    return np.exp(-np.linalg.norm(x-y)**2 / (2*sigma**2))\n",
    "\n",
    "def kernel_matrix(data, sigma):\n",
    "    \n",
    "    return compute_gram_matrix(data[:].reshape(-1, 1), data[:].reshape(-1, 1), sigma)\n",
    "\n",
    "def hsic_score(X, Y, sigma_x, sigma_y):\n",
    "    \"\"\"Compute HSIC using Gaussian kernels.\"\"\"\n",
    "    n = len(X)\n",
    "\n",
    "    # Kernel matrices\n",
    "    K = kernel_matrix(X, sigma_x)\n",
    "    L = kernel_matrix(Y, sigma_y)\n",
    "\n",
    "    # Centering matrix\n",
    "    H = np.eye(n) - np.ones((n, n)) / n\n",
    "\n",
    "    # Compute HSIC\n",
    "    \n",
    "    return (1.0 / (n-1)**2) * np.trace(np.dot(np.dot(np.dot(K, H), L), H))\n",
    "\n",
    "\n",
    "# def HSIC_spectrum(pick_data, sigma=.1, alpha=1e-5):\n",
    "    \n",
    "# #     gram_matrix_x = compute_gram_matrix(pick_data[:, 0].reshape(-1, 1), pick_data[:, 0].reshape(-1, 1), sigma)\n",
    "# #     gram_matrix_u = compute_gram_matrix(pick_data[:, 1].reshape(-1, 1), pick_data[:, 1].reshape(-1, 1), sigma)\n",
    "# #     gram_matrix_cross = compute_gram_matrix(pick_data[:, 0].reshape(-1, 1), pick_data[:, 1].reshape(-1, 1), sigma)\n",
    "\n",
    "# #     RX = torch.from_numpy(gram_matrix_x) + torch.eye(gram_matrix_x.shape[0])*alpha\n",
    "# #     RY = torch.from_numpy(gram_matrix_u) + torch.eye(gram_matrix_u.shape[0])*alpha\n",
    "# #     RXY = torch.from_numpy(gram_matrix_cross) \n",
    "\n",
    "# #     normalized_density = negative_half_power(RX)@RXY@negative_half_power(RY)\n",
    "# #     U, S, V = torch.linalg.svd(normalized_density)    \n",
    "    \n",
    "# #     # IMPLEMENT THE MEASURE\n",
    "\n",
    "    \n",
    "    \n",
    "#     return U, S, V\n",
    "\n",
    "def KICA_spectrum(data_x, data_y, alpha=1e-1):\n",
    "    gram_matrix_x = gauss(data_x, data_x, 0.001)\n",
    "    gram_matrix_u = gauss(data_y, data_y, 0.001)\n",
    "#     gram_matrix_cross = compute_gram_matrix(data_x, data_y, sigma)\n",
    "    \n",
    "    N0_matrix = (torch.eye(gram_matrix_x.shape[0]) - 1/gram_matrix_x.shape[0]).float()\n",
    "    \n",
    "    normalized_x = N0_matrix@gram_matrix_x@N0_matrix\n",
    "    normalized_u = N0_matrix@gram_matrix_u@N0_matrix\n",
    "\n",
    "    normalized_x_add = normalized_x + torch.eye((normalized_x.shape[0]))*alpha\n",
    "    normalized_u_add = normalized_u + torch.eye((normalized_u.shape[0]))*alpha\n",
    "\n",
    "    num_sample = data_x.shape[0]\n",
    "\n",
    "    matrix_a = np.zeros((num_sample*2, num_sample*2))\n",
    "\n",
    "    matrix_a[:num_sample, num_sample:] = normalized_x@normalized_u.T\n",
    "    matrix_a[num_sample:, :num_sample] = normalized_u@normalized_x.T\n",
    "\n",
    "    matrix_b = np.zeros((num_sample*2, num_sample*2))\n",
    "\n",
    "    matrix_b[:num_sample, :num_sample] = normalized_x_add@normalized_x_add.T\n",
    "    matrix_b[num_sample:, num_sample:] = normalized_u_add@normalized_u_add.T\n",
    "\n",
    "    eigenvalues, eigenvectors = eigh(matrix_a, matrix_b)\n",
    "    \n",
    "    spectrum_kica = eigenvalues[data_x.shape[0]:][::-1]\n",
    "    \n",
    "    \n",
    "    RF = normalized_x_add@normalized_x_add.T\n",
    "    RG = normalized_u_add@normalized_u_add.T\n",
    "    P = normalized_x@normalized_u.T\n",
    "\n",
    "    E1,V1 = torch.linalg.eigh(RF)\n",
    "    E2,V2 = torch.linalg.eigh(RG)\n",
    "\n",
    "    RF_NORM = V1@torch.diag(E1**(-1/2))@V1.T\n",
    "    RG_NORM = V2@torch.diag(E2**(-1/2))@V2.T\n",
    "    P_STAR = RF_NORM@P@RG_NORM\n",
    "\n",
    "    U, S, V = torch.svd(P_STAR)\n",
    "        \n",
    "    measure_kica = (-1/2)*np.log((1 - spectrum_kica[:]**2)).sum()\n",
    "#     measure_kica = np.linalg.det(P_STAR)\n",
    "    measure_hsic = np.trace(P_STAR)\n",
    "#     measure_hsic = np.sum(spectrum_kica[:]**2)\n",
    "\n",
    "    \n",
    "    return spectrum_kica, eigenvectors, U, S, V, measure_kica, measure_hsic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "073e2c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class decoder_MINER(nn.Module):\n",
    "    def __init__(self, input_dim = 784, HIDDEN = 2000, out_dim = 200):\n",
    "        super(decoder_MINER, self).__init__()\n",
    "        self.dim = out_dim\n",
    "    \n",
    "#         self.fc1 = nn.Linear(input_dim+20, HIDDEN, bias=True)\n",
    "        self.fc1 = nn.Linear(input_dim, HIDDEN, bias=True)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(HIDDEN)\n",
    "        self.fc2 = nn.Linear(HIDDEN, HIDDEN, bias=True)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(HIDDEN)\n",
    "        self.fc3 = nn.Linear(HIDDEN, HIDDEN, bias=True)\n",
    "        self.bn3 = torch.nn.BatchNorm1d(HIDDEN)\n",
    "        self.fc4 = nn.Linear(HIDDEN, HIDDEN, bias=True)\n",
    "        self.bn4 = torch.nn.BatchNorm1d(HIDDEN)\n",
    "        self.fc6 = nn.Linear(HIDDEN, HIDDEN, bias=True)\n",
    "        self.bn6 = torch.nn.BatchNorm1d(HIDDEN)\n",
    "        self.fc7 = nn.Linear(HIDDEN, HIDDEN, bias=True)\n",
    "        self.bn7 = torch.nn.BatchNorm1d(HIDDEN)\n",
    "\n",
    "        self.fc8 = nn.Linear(HIDDEN, HIDDEN, bias=True)\n",
    "        self.bn8 = torch.nn.BatchNorm1d(HIDDEN)\n",
    "        self.fc9 = nn.Linear(HIDDEN, HIDDEN, bias=True)\n",
    "        self.bn9 = torch.nn.BatchNorm1d(HIDDEN)\n",
    "\n",
    "        self.fc5 = nn.Linear(HIDDEN, out_dim, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "#         x = torch.cat((x, torch.zeros((x.shape[0], 20)).uniform_().cuda()), 1)\n",
    "        \n",
    "        x = torch.relu(((self.fc1(x))))\n",
    "        x = torch.relu(((self.fc2(x))))\n",
    "        x = torch.relu(((self.fc3(x))))\n",
    "        x = torch.relu(((self.fc4(x))))\n",
    "        x = torch.relu(((self.fc6(x))))\n",
    "#         x = torch.relu(self.bn7((self.fc7(x))))\n",
    "#         x = torch.relu(self.bn8((self.fc8(x))))\n",
    "#         x = torch.relu(self.bn9((self.fc9(x))))\n",
    "\n",
    "        x = torch.sigmoid(self.fc5(x))+1e-1\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bdad92d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class decoder_MINES(nn.Module):\n",
    "    def __init__(self, input_dim = 784, HIDDEN = 2000, out_dim = 200):\n",
    "        super(decoder_MINES, self).__init__()\n",
    "        self.dim = out_dim\n",
    "    \n",
    "#         self.fc1 = nn.Linear(input_dim+20, HIDDEN, bias=True)\n",
    "        self.fc1 = nn.Linear(input_dim, HIDDEN, bias=True)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(HIDDEN)\n",
    "        self.fc2 = nn.Linear(HIDDEN, HIDDEN, bias=True)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(HIDDEN)\n",
    "        self.fc3 = nn.Linear(HIDDEN, HIDDEN, bias=True)\n",
    "        self.bn3 = torch.nn.BatchNorm1d(HIDDEN)\n",
    "        self.fc4 = nn.Linear(HIDDEN, HIDDEN, bias=True)\n",
    "        self.bn4 = torch.nn.BatchNorm1d(HIDDEN)\n",
    "        self.fc6 = nn.Linear(HIDDEN, HIDDEN, bias=True)\n",
    "        self.bn6 = torch.nn.BatchNorm1d(HIDDEN)\n",
    "        self.fc7 = nn.Linear(HIDDEN, HIDDEN, bias=True)\n",
    "        self.bn7 = torch.nn.BatchNorm1d(HIDDEN)\n",
    "\n",
    "        self.fc8 = nn.Linear(HIDDEN, HIDDEN, bias=True)\n",
    "        self.bn8 = torch.nn.BatchNorm1d(HIDDEN)\n",
    "        self.fc9 = nn.Linear(HIDDEN, HIDDEN, bias=True)\n",
    "        self.bn9 = torch.nn.BatchNorm1d(HIDDEN)\n",
    "\n",
    "        self.fc5 = nn.Linear(HIDDEN, out_dim, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "#         x = torch.cat((x, torch.zeros((x.shape[0], 20)).uniform_().cuda()), 1)\n",
    "        \n",
    "        x = torch.relu(((self.fc1(x))))\n",
    "        x = torch.relu(((self.fc2(x))))\n",
    "        x = torch.relu(((self.fc3(x))))\n",
    "        x = torch.relu(((self.fc4(x))))\n",
    "        x = torch.relu(((self.fc6(x))))\n",
    "#         x = torch.relu(self.bn7((self.fc7(x))))\n",
    "#         x = torch.relu(self.bn8((self.fc8(x))))\n",
    "#         x = torch.relu(self.bn9((self.fc9(x))))\n",
    "\n",
    "        x = (self.fc5(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7af89b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class encoder(nn.Module):\n",
    "    def __init__(self, input_dim = 784, HIDDEN = 2000, out_dim = 200):\n",
    "        super(encoder, self).__init__()\n",
    "        self.dim = out_dim\n",
    "    \n",
    "        self.fc1 = nn.Linear(input_dim+50, HIDDEN, bias=True)\n",
    "#         self.fc1 = nn.Linear(input_dim, HIDDEN, bias=True)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(HIDDEN)\n",
    "        self.fc2 = nn.Linear(HIDDEN, HIDDEN, bias=True)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(HIDDEN)\n",
    "        self.fc3 = nn.Linear(HIDDEN, HIDDEN, bias=True)\n",
    "        self.bn3 = torch.nn.BatchNorm1d(HIDDEN)\n",
    "        self.fc4 = nn.Linear(HIDDEN, HIDDEN, bias=True)\n",
    "        self.bn4 = torch.nn.BatchNorm1d(HIDDEN)\n",
    "        self.fc6 = nn.Linear(HIDDEN, HIDDEN, bias=True)\n",
    "        self.bn6 = torch.nn.BatchNorm1d(HIDDEN)\n",
    "        self.fc7 = nn.Linear(HIDDEN, HIDDEN, bias=True)\n",
    "        self.bn7 = torch.nn.BatchNorm1d(HIDDEN)\n",
    "        \n",
    "        self.fc8 = nn.Linear(HIDDEN, HIDDEN, bias=True)\n",
    "        self.bn8 = torch.nn.BatchNorm1d(HIDDEN)\n",
    "        self.fc9 = nn.Linear(HIDDEN, HIDDEN, bias=True)\n",
    "        self.bn9 = torch.nn.BatchNorm1d(HIDDEN)\n",
    "\n",
    "        self.fc5 = nn.Linear(HIDDEN, out_dim, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = torch.cat((x, torch.zeros((x.shape[0], 50)).uniform_().cuda()), 1)\n",
    "        \n",
    "        x = torch.relu(self.bn1((self.fc1(x))))\n",
    "        x = torch.relu(self.bn2((self.fc2(x))))\n",
    "        x = torch.relu(self.bn3((self.fc3(x))))\n",
    "        x = torch.relu(self.bn4((self.fc4(x))))\n",
    "        x = torch.relu(self.bn6((self.fc6(x))))\n",
    "        x = torch.relu(self.bn7((self.fc7(x))))\n",
    "        x = torch.relu(self.bn8((self.fc8(x))))\n",
    "        x = torch.relu(self.bn9((self.fc9(x))))\n",
    "\n",
    "        x = torch.sigmoid(self.fc5(x))\n",
    "\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59476a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for mine_index in [0, 1]:\n",
    "    for set_index in [0, 1, 2]:\n",
    "        \n",
    "        \n",
    "        torch.cuda.set_device(4)\n",
    "\n",
    "        def gauss(A,B,var):\n",
    "            return torch.exp(-((A.unsqueeze(1) - B.unsqueeze(0))**2).mean(2)/(2*var))\n",
    "\n",
    "        # train_data = run_dataset[1]\n",
    "\n",
    "        train_data = save_dataset[set_index]\n",
    "\n",
    "        torch.manual_seed(0)\n",
    "        np.random.seed(0)\n",
    "\n",
    "        E = encoder(input_dim = 2, out_dim = 1).cuda()\n",
    "#         D = decoder(input_dim = 3, out_dim = 1).cuda()\n",
    "\n",
    "            \n",
    "            \n",
    "        if mine_index == 0:\n",
    "            D = decoder_MINES(input_dim = 3, out_dim = 1).cuda()\n",
    "        \n",
    "        if mine_index == 1:\n",
    "            D = decoder_MINER(input_dim = 3, out_dim = 1).cuda()\n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "        optimizer_E = optim.Adam([\n",
    "              {'params': E.parameters(), 'lr': 0.00001, 'betas': (0.5, 0.9)},\n",
    "           ])\n",
    "\n",
    "        optimizer_D = optim.Adam([\n",
    "              {'params': D.parameters(), 'lr': 0.00001, 'betas': (0.5, 0.9)},\n",
    "           ])\n",
    "\n",
    "        elbo_curve = []\n",
    "        pdf_curve = []\n",
    "\n",
    "        data_dim = 2\n",
    "        center_dim = 30\n",
    "\n",
    "        for i in range(0, 20001):\n",
    "\n",
    "            batch_data = torch.from_numpy(train_data[:5000]).float().cuda()\n",
    "\n",
    "            # apply encoders\n",
    "\n",
    "            encoded = E(batch_data)\n",
    "\n",
    "            # add multiple \"noise\" or just one it is fine\n",
    "\n",
    "            var_noise = 0.001\n",
    "            var_samples = 0.001\n",
    "\n",
    "            batch_noise = batch_data + torch.zeros((batch_data.shape)).cuda().normal_()*np.sqrt(var_noise)\n",
    "            encoded_noise = encoded + torch.zeros((encoded.shape)).cuda().normal_()*np.sqrt(var_noise)\n",
    "\n",
    "            index = np.arange(5000)\n",
    "            np.random.shuffle(index)\n",
    "            index2 = np.arange(5000)\n",
    "            np.random.shuffle(index2)\n",
    "\n",
    "            joint = torch.cat((batch_noise.cuda(), encoded_noise.cuda()), 1)\n",
    "            disjoint = torch.cat((batch_noise[index].cuda(), encoded_noise[index2].cuda()), 1)\n",
    "\n",
    "            output_joint = D(joint)\n",
    "            output_uniform = D(disjoint)\n",
    "            \n",
    "            \n",
    "            if mine_index == 0:\n",
    "                error = torch.mean(output_joint) - torch.log(torch.mean(torch.exp(output_uniform)))\n",
    "            if mine_index == 1:\n",
    "                error = output_joint.mean()/(torch.sqrt((output_uniform**2).mean()))\n",
    "\n",
    "            \n",
    "            (-error).backward()\n",
    "\n",
    "            pdf_curve.append(error.item())\n",
    "\n",
    "            optimizer_E.step()\n",
    "            optimizer_D.step()\n",
    "\n",
    "            optimizer_E.zero_grad()\n",
    "            optimizer_D.zero_grad()\n",
    "\n",
    "            if i%100 == 0:\n",
    "                print(i, error.item())\n",
    "\n",
    "#                 if i % 500 == 0:\n",
    "#                     grid_size = 50\n",
    "#                     x = np.linspace(0.0, 1, grid_size)\n",
    "#                     y = np.linspace(0.0, 1, grid_size)\n",
    "#                     X, Y = np.meshgrid(x, y)\n",
    "#                     grid_points = np.column_stack([X.flatten(), Y.flatten()])\n",
    "#                     grid_points = torch.from_numpy(grid_points)\n",
    "\n",
    "#                     output_mean = 0\n",
    "\n",
    "#                     for n in range(0, 100):\n",
    "\n",
    "#                         E.eval()\n",
    "#                         with torch.no_grad():\n",
    "#                             output = E(grid_points.cuda().float()).detach().cpu().numpy()\n",
    "#                         E.train() \n",
    "\n",
    "#                         output_mean = (output_mean*n+output)/(n+1)\n",
    "\n",
    "#                     plt.figure(figsize=(3, 3))\n",
    "#                     heatmap_extent = [0, 1, 0, 1]\n",
    "\n",
    "#                     plt.imshow(-output_mean.reshape(-1).reshape(50, 50), cmap='coolwarm', extent=[0, 1, 0, 1], origin='lower', aspect='auto', zorder=1)\n",
    "#                     plt.axis('off') \n",
    "#                     plt.show()\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "        output_mean = 0\n",
    "\n",
    "        grid_size = 50\n",
    "        x = np.linspace(0.0, 1, grid_size)\n",
    "        y = np.linspace(0.0, 1, grid_size)\n",
    "        X, Y = np.meshgrid(x, y)\n",
    "        grid_points = np.column_stack([X.flatten(), Y.flatten()])\n",
    "        grid_points = torch.from_numpy(grid_points)\n",
    "\n",
    "        for n in range(0, 100):\n",
    "\n",
    "            E.eval()\n",
    "            with torch.no_grad():\n",
    "                output = E(grid_points.cuda().float()).detach().cpu().numpy()\n",
    "            E.train() \n",
    "\n",
    "            output_mean = (output_mean*n+output)/(n+1)\n",
    "\n",
    "        plt.figure(figsize=(3, 3))\n",
    "        heatmap_extent = [0, 1, 0, 1]\n",
    "\n",
    "\n",
    "        plt.imshow(output_mean.reshape(-1).reshape(50, 50), cmap='coolwarm', extent=[0, 1, 0, 1], origin='lower', aspect='auto', zorder=1)\n",
    "        plt.axis('off')\n",
    "        plt.savefig('./max_dataset/MI_set_{0}_center_{1}_negative.png'.format(set_index, set_inf), dpi=500, bbox_inches='tight')\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "        plt.figure(figsize=(3, 3))\n",
    "        heatmap_extent = [0, 1, 0, 1]\n",
    "\n",
    "\n",
    "        plt.imshow(-output_mean.reshape(-1).reshape(50, 50), cmap='coolwarm', extent=[0, 1, 0, 1], origin='lower', aspect='auto', zorder=1)\n",
    "        plt.axis('off')\n",
    "        plt.savefig('./max_dataset/MI_set_{0}_center_{1}.png'.format(set_index, set_inf), dpi=500, bbox_inches='tight')\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "                    \n",
    "                    \n",
    "        spectrum_kica, eigenvectors, U, S, V, measure_kica, measure_hsic = KICA_spectrum(batch_data.detach().cpu(), encoded.detach().cpu(), alpha=1)\n",
    "\n",
    "        np.save('./max_dataset/measure_kica_{0}_figure_MINEMINE_{1}.npy'.format(set_index, mine_index), measure_kica)\n",
    "        np.save('./max_dataset/measure_hsic_{0}_figure_MINEMINE_{1}.npy'.format(set_index, mine_index), measure_hsic)\n",
    "        np.save('./max_dataset/pdf_array_{0}_MINEMINE_{1}.npy'.format(set_index, mine_index), np.array(pdf_curve))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888e8e01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
